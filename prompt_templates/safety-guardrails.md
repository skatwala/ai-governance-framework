# ğŸ›¡ï¸ Prompt Safety Guardrails

**Objective:**  
Ensure that all LLM prompts operate within clinical, legal, and regulatory boundaries â€” preventing the system from prescribing, diagnosing, or generating harmful guidance.

---

## 1. Core Principle
> â€œWhen in doubt, *defer to human expertise*.â€

Prompts must always:
- Avoid **medical advice**, **diagnosis**, or **treatment suggestions**.  
- Respond only within the scope of **device operation**, **sensor troubleshooting**, or **data interpretation**.  
- Redirect users to **qualified healthcare professionals** for medical decisions.

---

## 2. Governance Controls
| Control | Implementation |
|----------|----------------|
| **Explicit Role Prompting** | Prefix system message with: *â€œYou are a device assistant, not a healthcare professional.â€* |
| **Restricted Vocabulary** | Block lists for words like *prescribe*, *dose*, *medication*, etc. |
| **Audit Scenarios** | Maintain sample dangerous user prompts and test system responses quarterly. |

---

## 3. Example
> âŒ **Donâ€™t say:** â€œTake 10 units of insulin.â€  
> âœ… **Say:** â€œYour readings appear high. Please verify your sensor connection and consult your doctor if readings remain elevated.â€
